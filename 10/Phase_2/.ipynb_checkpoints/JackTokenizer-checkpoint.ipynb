{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e62dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db7da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JackTokenizer:\n",
    "    keywordType = ['class', 'constructor', 'function', 'method', 'field', 'static', 'var', 'int', 'char', 'boolean',\n",
    "                   'void', 'true', 'false', 'null', 'this', 'let', 'do', 'if', 'else', 'while', 'return']\n",
    "    symbolType = ['{', '}', '(', ')', '[', ']', '.', ',', ';', '+', '-', '*', '/', '&', '|', '<', '>', '=', '~',\n",
    "                  '&lt;', '&gt;', '&amp;']\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.data = [] # 源文件\n",
    "        self.tokens = [] # 处理后的字元和所在行\n",
    "        self.length = 0 # tokens的长度\n",
    "        self.pc = -1 # 当前处理的字元下标\n",
    "        self.Type = '' # 当前处理的字元的类型\n",
    "        \n",
    "        # 读入文件内容并删除注释\n",
    "        self.data = self.delNotes(file).split('\\n')\n",
    "        # 分割字元\n",
    "        self.tokens = self.divideTokens(self.data)\n",
    "        self.length = len(self.tokens)\n",
    "    \n",
    "    def delNotes(self, file):\n",
    "        _map = { }\n",
    "        outstring = ''\n",
    "\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            while True:\n",
    "                m = re.compile('\\\".*?\\\"')\n",
    "                # 每次只查找一个并替换\n",
    "                _str = m.search( line )\n",
    "                # 如果没匹配成功，就合并，然后下一行\n",
    "                if None == _str:\n",
    "                    outstring += line\n",
    "                    break\n",
    "                # 如果匹配成功了，将字符串替换为uuid\n",
    "                key = str( uuid.uuid1() ) # 通用唯一识别码\n",
    "                m = re.compile('\\\".*?\\\"')\n",
    "                # 将line中的串模式m替换为key\n",
    "                outtmp = re.sub(m, key, line)\n",
    "                line = outtmp\n",
    "                # 将 uuid:字符串 放入字典\n",
    "                _map[ key ] = _str.group() # group() -- 匹配的整个表达式的字符串\n",
    "            line = file.readline()\n",
    "\n",
    "        # 删除单行注释\n",
    "        m = re.compile(r'//.*')\n",
    "        outtmp = re.sub(m, '', outstring)\n",
    "        outstring = outtmp\n",
    "\n",
    "        # 删除多行注释\n",
    "        m = re.compile(r'/\\*.*?\\*/', re.S)\n",
    "        outtmp = re.sub(m, '', outstring)\n",
    "        outstring = outtmp\n",
    "\n",
    "        # 将字符串恢复\n",
    "        for key in _map.keys():\n",
    "            outstring = outstring.replace(key, _map[key])\n",
    "\n",
    "        return outstring\n",
    "    \n",
    "    def divideTokens(self, data):\n",
    "        tokens = []\n",
    "        lineIndex = 0\n",
    "        # 按行进行处理\n",
    "        for line in data:\n",
    "            lineIndex += 1 # 行数加1\n",
    "            inStr = False\n",
    "            token = ''\n",
    "            for char in line:\n",
    "                if inStr == False: # 如果没遇到第一个\"\n",
    "                    if re.match('\\w|_', char) != None: # 如果是字符或下划线\n",
    "                        token += char # 放入字元\n",
    "                    if char == ' ' and token != '': # 如果遇到空格而且字元不为空\n",
    "                        tokens.append([token, lineIndex]) # 把字元放入字元列表\n",
    "                        token = '' # 清空字元\n",
    "                    if char in self.symbolType: # 如果遇到符号\n",
    "                        if char == '>': # 转换三种符号\n",
    "                            char = '&gt;'\n",
    "                        if char == '<':\n",
    "                            char = '&lt;'\n",
    "                        if char == '&':\n",
    "                            char = '&amp;'\n",
    "                        if token != '':\n",
    "                            tokens.append([token, lineIndex])\n",
    "                            token = ''\n",
    "                        tokens.append([char, lineIndex])\n",
    "                    if char == '\"':\n",
    "                        inStr = True\n",
    "                        token += char\n",
    "                else: # 如果当前是字符串，所有的字符都将放入字元，直至遇到下一个\"\n",
    "                    token += char\n",
    "                    if char == '\"':\n",
    "                        inStr = False\n",
    "                        tokens.append([token, lineIndex])\n",
    "                        token = ''\n",
    "            if token != '':\n",
    "                tokens.append([token, lineIndex])\n",
    "        return tokens\n",
    "    \n",
    "    def hasMoreTokens(self):\n",
    "        return self.pc < self.length - 1\n",
    "    \n",
    "    def advance(self):\n",
    "        self.pc = self.pc + 1\n",
    "    \n",
    "    def tokenType(self):\n",
    "        self.Type = ''\n",
    "        token = self.tokens[self.pc][0]\n",
    "        lineIndex = self.tokens[self.pc][1]\n",
    "        \n",
    "        if token in self.keywordType: # 关键字识别一定要在标识符之前\n",
    "            self.Type = 'KEYWORD'\n",
    "            return 'KEYWORD'\n",
    "        if token in self.symbolType:\n",
    "            self.Type = 'SYMBOL'\n",
    "            return 'SYMBOL'\n",
    "        if re.match(r'^[_a-zA-Z][_a-zA-Z0-9]*$', token):\n",
    "            self.Type = 'IDENTIFIER'\n",
    "            return 'IDENTIFIER'\n",
    "        if re.match(r'^\\d+$', token) and 0 <= int(token) <= 32767:\n",
    "            self.Type = 'INT_CONST'\n",
    "            return 'INT_CONST'\n",
    "        if re.match(r'^\".*\"$', token):\n",
    "            self.Type = 'STRING_CONST'\n",
    "            return 'STRING_CONST' \n",
    "        raise Exception(\"\\n--> \" + str(lineIndex-1) + ' ' + self.data[lineIndex-2] + \n",
    "                        \"\\n--> \" + str(lineIndex) + ' ' + self.data[lineIndex-1] + '    <-- Syntax Error:' + token + \n",
    "                        \"\\n--> \" + str(lineIndex+1) + ' ' + self.data[lineIndex])\n",
    "    \n",
    "    def keyword(self):\n",
    "        if self.Type == 'KEYWORD':\n",
    "            return self.tokens[self.pc] # 返回关键字及所在行\n",
    "        raise Exception(\"KEYWORD_类型不符\")\n",
    "    \n",
    "    def symbol(self):\n",
    "        if self.Type == 'SYMBOL':\n",
    "            return self.tokens[self.pc]\n",
    "        raise Exception(\"SYMBOL_类型不符\")\n",
    "    \n",
    "    def identifier(self):\n",
    "        if self.Type == 'IDENTIFIER':\n",
    "            return self.tokens[self.pc]\n",
    "        raise Exception(\"INT_CONST_类型不符\")\n",
    "    \n",
    "    def intVal(self):\n",
    "        if self.Type == 'INT_CONST':\n",
    "            return self.tokens[self.pc]\n",
    "        raise Exception(\"STRING_CONST_类型不符\")\n",
    "    \n",
    "    def stringVal(self):\n",
    "        if self.Type == 'STRING_CONST':\n",
    "            return [self.tokens[self.pc][0][1:-1], self.tokens[self.pc][1]]\n",
    "        raise Exception(\"IDENTIFIER_类型不符\")\n",
    "        \n",
    "    def getTokens(self):\n",
    "        tags_tokens = [] # 标签、字元、所在行\n",
    "        while(self.hasMoreTokens()):\n",
    "            self.advance()\n",
    "            if self.tokenType() == 'KEYWORD':\n",
    "                tags_tokens.append(['keyword', self.keyword()[0], str(self.keyword()[1])])\n",
    "            if self.tokenType() == 'SYMBOL':\n",
    "                tags_tokens.append(['symbol', self.symbol()[0], str(self.symbol()[1])])\n",
    "            if self.tokenType() == 'IDENTIFIER':\n",
    "                tags_tokens.append(['identifier', self.identifier()[0], str(self.identifier()[1])])\n",
    "            if self.tokenType() == 'INT_CONST':\n",
    "                tags_tokens.append(['integerConstant', self.intVal()[0], str(self.intVal()[1])])\n",
    "            if self.tokenType() == 'STRING_CONST':\n",
    "                tags_tokens.append(['stringConstant', self.stringVal()[0], str(self.stringVal()[1])])\n",
    "        \n",
    "        return tags_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
